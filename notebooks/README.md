# DSPy Optimizer Benchmark Framework

A modular, config-driven framework for benchmarking DSPy prompt optimizers (GEPA, MIPROv2, COPRO, etc.) on various datasets.

## Quick Start

```bash
# Run a baseline evaluation
python scripts/run_experiment.py configs/experiments/hotpotqa_baseline.yaml

# Run GEPA optimization
python scripts/run_experiment.py configs/experiments/hotpotqa_gepa_v1.yaml

# Run with detailed analysis and visualizations
python scripts/run_experiment_with_analysis.py configs/experiments/hotpotqa_gepa_v1.yaml
```

## Architecture

### Modular Configuration System

Configs are organized into reusable, composable modules to ensure consistency and reduce duplication:

```
configs/
├── base/
│   └── logging.yaml              # Common logging settings
├── datasets/
│   └── hotpotqa_standard.yaml    # Dataset configurations
├── models/
│   └── qwen3_4b.yaml             # Model configurations
├── optimizers/
│   ├── baseline.yaml             # No optimization
│   ├── gepa.yaml                 # GEPA settings
│   └── mipro.yaml                # MIPROv2 settings
├── programs/
│   └── mlflow_base.yaml          # Program/signature configs
└── experiments/
    ├── hotpotqa_baseline.yaml    # Composed experiments
    ├── hotpotqa_gepa_v1.yaml
    └── hotpotqa_mipro_v1.yaml
```

### Benefits

- **Single Source of Truth**: Change dataset settings once, all experiments update
- **Consistency**: All experiments use identical base configurations
- **Easy Comparison**: Only differences between experiments are visible
- **Quick Testing**: Create variants without copying boilerplate

## Creating New Experiments

### Example: Experiment Config

```yaml
# configs/experiments/my_experiment.yaml
name: my_experiment
description: Testing GEPA with different settings

# Reference shared configs
dataset_ref: hotpotqa_standard
model_ref: qwen3_4b
optimizer_ref: gepa
program_ref: mlflow_base
logging_ref: logging

# Override only what's different
optimizer_overrides:
  params:
    max_metric_calls: 300  # Run longer
    
logging_overrides:
  save_models: true
```

### Creating New Shared Configs

**Dataset variant** (`configs/datasets/hotpotqa_small.yaml`):
```yaml
name: hotpotqa
train_size: 50
dev_size: 10
test_size: 0
train_seed: 1
eval_seed: 2023
keep_details: true
params:
  use_context: true
```

**Model variant** (`configs/models/llama3_8b.yaml`):
```yaml
name: ollama_chat/llama3:8b
api_base: http://localhost:11434
cache: true
params: {}
```

Then reference them in experiments:
```yaml
dataset_ref: hotpotqa_small  # Use smaller dataset
model_ref: llama3_8b          # Use different model
```

## Configuration Reference

### Dataset Configuration

```yaml
dataset:
  name: hotpotqa                  # Dataset name
  train_size: 200                 # Training examples for optimization
  dev_size: 20                    # Validation examples for tracking
  test_size: 0                    # Test set size (0 = no test)
  train_seed: 1                   # Random seed for train split
  eval_seed: 2023                 # Random seed for eval split
  keep_details: true              # Keep additional metadata
  params:
    use_context: true             # Dataset-specific parameters
```

### Model Configuration

```yaml
model:
  name: ollama_chat/qwen3:4b-instruct
  api_base: http://localhost:11434
  cache: true                     # Enable response caching
  params: {}                      # Additional model parameters
```

### Optimizer Configuration

**GEPA**:
```yaml
optimizer:
  name: gepa
  auto: light                     # Optimization mode
  num_threads: 1                  # Parallel threads
  params:
    reflection_lm: null           # null = use main model
    reflection_minibatch_size: 1  # Examples per reflection (1-3)
    max_metric_calls: 200         # Max optimization iterations
    track_stats: true             # Track detailed statistics
```

**Baseline** (no optimization):
```yaml
optimizer:
  name: baseline
  auto: light
  num_threads: 1
  params: {}
```

### Key Parameters for GEPA

| Parameter | Default | Recommended | Description |
|-----------|---------|-------------|-------------|
| `reflection_minibatch_size` | 3 | **1-2** for small models | Examples in reflection prompt |
| `train_size` | 50 | 100-300 | Training examples |
| `dev_size` | 100 | 10-50 | Validation examples |
| `max_metric_calls` | 200 | 200-500 | Max iterations |
| `cache` | true | false when debugging | Enable/disable caching |

### Context Window Considerations

⚠️ **Important**: HotPotQA examples contain long context passages. The reflection prompt sent to the reflection LM includes:
- Current instruction
- Multiple examples with full context, question, answer, and feedback

**Prompt size estimation**:
- Each HotPotQA example in the reflection prompt: ~5,000-15,000 characters
- With `reflection_minibatch_size=3`: prompts can exceed 20,000+ characters (~5,000-7,000 tokens)

**Recommendations by model size**:

| Model Size | `reflection_minibatch_size` | Notes |
|------------|----------------------------|-------|
| 4B-7B | 1 | Keep prompts short; may still truncate on long examples |
| 13B-30B | 1-2 | Better handling of longer contexts |
| 70B+ / API | 2-3 | Can handle full reflection prompts |

### Program Configuration

```yaml
program:
  name: mlflow_base_prompt        # Program type
  params: {}                      # Program-specific parameters
```

Available programs (see `programs/` directory):
- `mlflow_base_prompt`: Context-aware QA with pre-defined template
- `naive`: Direct question → answer
- `cot`: Chain-of-thought reasoning

### Troubleshooting

#### Empty or truncated instruction proposals

**Symptom**: GEPA logs show `Proposed new text for answer:` with empty or very short text.

**Causes**:
1. Reflection prompt too long for model's effective context
2. Model returns only code fences (````) without content

**Solutions**:
- Reduce `--reflection_minibatch_size` to 1
- Use a larger model for reflection (`--reflection_lm`)
- Add `max_tokens=4096` to the reflection LM (already configured)

#### "All subsample scores perfect. Skipping."

**Symptom**: GEPA skips reflection because the minibatch scored perfectly.

**Cause**: With small `reflection_minibatch_size`, you may randomly sample only "easy" examples.

**Solutions**:
- Increase `--train_size` for more diverse sampling
- Set `skip_perfect_score=False` in GEPA config (requires code change)

#### Proposals don't improve scores

**Symptom**: New instructions score worse than baseline on validation set.

**Cause**: The reflection LM may propose overly specific or incorrect heuristics based on the small minibatch.

**Solutions**:
- Increase `--train_size` for better coverage
- Use a stronger reflection model
- Run longer with more `max_metric_calls`

## Running Experiments

### Basic Execution

```bash
# Run a single experiment
python scripts/run_experiment.py configs/experiments/hotpotqa_baseline.yaml

# Run with analysis and visualizations
python scripts/run_experiment_with_analysis.py configs/experiments/hotpotqa_gepa_v1.yaml
```

### Results

Results are saved to:
- **Logs**: `results/logs/` - Detailed execution logs
- **Models**: `results/models/` - Optimized programs (if `save_models: true`)
- **Reports**: `results/reports/` - Analysis reports and plots

### Example Workflow

```bash
# 1. Run baseline
python scripts/run_experiment.py configs/experiments/hotpotqa_baseline.yaml

# 2. Run GEPA optimization
python scripts/run_experiment_with_analysis.py configs/experiments/hotpotqa_gepa_v1.yaml

# 3. Compare results (check results/logs/ for JSON files)
```

## Output Interpretation

### GEPA Candidate Inspection

After optimization, the script prints all proposed candidates:

```
============================================================
GEPA CANDIDATE INSPECTION
============================================================
Total candidates explored: 5
Best candidate index: 0
Best validation score: 0.7263

[Rank 1] Candidate 0 — Score: 0.7263 ★ BEST
  [answer]:
    You are a question answering assistant...

[Rank 2] Candidate 1 — Score: 0.4164
  [answer]:
    Given the fields `context`, `question`, produce the fields `answer`.
```

- **Candidate 0** is always the original/baseline
- If Candidate 0 remains best, optimization found no improvements
- Candidates with default DSPy signature (`Given the fields...`) indicate empty proposals

### Score Distribution

```
SCORE DISTRIBUTION
  Min:    0.3520
  Max:    0.5411
  Mean:   0.3917
```

- **High variance** (large Max-Min gap): proposals are exploring diverse strategies
- **Low mean with high max**: some proposals work well, others don't
- **All similar scores**: optimization is stuck or baseline is already optimal

## Project Structure

```
notebooks/
├── configs/                      # Configuration files
│   ├── base/                    # Base configs (logging, etc.)
│   ├── datasets/                # Dataset configurations
│   ├── models/                  # Model configurations
│   ├── optimizers/              # Optimizer configurations
│   ├── programs/                # Program configurations
│   └── experiments/             # Experiment definitions
├── core/                        # Core framework
│   ├── config.py               # Config loading & composition
│   ├── experiment.py           # Experiment runner
│   ├── logging.py              # Logging utilities
│   ├── metrics.py              # Evaluation metrics
│   └── analysis.py             # Result analysis
├── programs/                    # DSPy program definitions
│   ├── base.py                 # Base program interface
│   └── qa.py                   # QA program implementations
├── data_adapters/              # Dataset loaders
│   └── hotpotqa_adapter.py    # HotPotQA loader
├── optimizers/                  # Optimizer wrappers
│   ├── baseline_optimizer.py
│   ├── gepa_optimizer.py
│   └── mipro_optimizer.py
├── scripts/                     # Entry point scripts
│   ├── run_experiment.py       # Basic runner
│   └── run_experiment_with_analysis.py  # Runner with analysis
└── results/                     # Output directory
    ├── logs/                   # Execution logs
    ├── models/                 # Saved models
    └── reports/                # Analysis reports
```

## Tips & Best Practices

### Configuration Tips

1. **Consistent Comparisons**: Always use the same `dataset_ref` for experiments you want to compare
2. **Quick Testing**: Create a `*_small.yaml` dataset config with reduced sizes for rapid iteration
3. **Version Control**: The small experiment configs make git diffs very readable
4. **Documentation**: Use descriptive names and include `description` fields

### Performance Tuning

1. **Small Models (4B-7B)**:
   - `reflection_minibatch_size: 1`
   - `train_size: 50-100`
   - `dev_size: 10-20`

2. **Medium Models (13B-30B)**:
   - `reflection_minibatch_size: 1-2`
   - `train_size: 100-200`
   - `dev_size: 20-50`

3. **Large Models/APIs (70B+)**:
   - `reflection_minibatch_size: 2-3`
   - `train_size: 200-500`
   - `dev_size: 50-100`

